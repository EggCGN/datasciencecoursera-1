---
title: "Week 1 assignment"
output: html_notebook
---

# Memo

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

# Week 1 quiz

### Question 1

The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv

and load the data into R. The code book, describing the variable names is here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf

How many properties are worth $1,000,000 or more?

```{r}
link <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
file_path <- "/tmp/housing_Idaho.csv"
download.file(link, file_path)
print(date())
```
```{r}
library(data.table)
dt <- fread(file_path)
dt[, .N, VAL >= 24]
```

Answer: there are 53 properties above $1,000,000.

### Question 2

Use the data you loaded from Question 1. Consider the variable FES in the code book. Which of the "tidy data" principles does this variable violate?

Answer: tidy data has one variable per column

### Question 3

Download the Excel spreadsheet on Natural Gas Aquisition Program here:
https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx
Read rows 18-23 and columns 7-15 into R and assign the result to a variable called: dat

What is the value of: sum(dat$Zip*dat$Ext,na.rm=T)

```{r}
link <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
file_path <- "/tmp/gov_NGPA.xlsx"
download.file(link, file_path)
print(date())
```

```{r}
library(xlsx)
dat <- read.xlsx(file_path, sheetIndex=1, colIndex=7:15, rowIndex=18:23)
sum(dat$Zip*dat$Ext,na.rm=T)
```

Answer: 36534720

### Question 4

Read the XML data on Baltimore restaurants from here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml

How many restaurants have zipcode 21231?

```{r}
library(XML)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileURL, useInternal=TRUE)
rootNode <- xmlRoot(doc)
print(date())
```

```{r}
length(xpathSApply(rootNode, "//raw[@zipcode=21231]", xmlValue))
```

This question has not been tested due to sml lib limitation on the current computer.

### Question 5

The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:

https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv

using the fread() command load the data into an R object named "DT"

The following are ways to calculate the average value of the variable "pwgt15" broken down by sex. Using the data.table package, which will deliver the fastest user time?

```{r}
link <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
file_path <- "/tmp/housing_Idaho.csv"
download.file(link, file_path)
print(date())
```

```{r}
library(data.table)
DT <- fread(file_path)
system.time(tapply(DT$pwgtp15, DT$SEX, mean))
Sys.sleep(0.5)
system.time(sapply(split(DT$pwgtp15, DT$SEX), mean))
Sys.sleep(0.5)
system.time(DT[,mean(pwgtp15), by=SEX])
Sys.sleep(0.5)
system.time(mean(DT$pwgtp15, by=DT$SEX))
```

The answer is *DT[,mean(pwgtp15), by=SEX]* as said in the course, but during the above test, we can see that this is the longest.